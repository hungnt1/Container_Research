

## Service discovery

- Trong K8S, để cân bằng tải giữa các service và để liên hệ với nhau giữa các service. Ingress ojbect được khởi tạo để truy cập vào cluster từ bên ngoài


## Service

- Các pod trong k8s đều có vòng đời, chúng có thể khởi tạo và xóa bất cứ lúc nào, nhưng khi đã được phá hủy Pod được coi như biến mất hoàn toàn. Bằng cách sử dụng ReplicationController cung cấp khả năng linh động cho việc khởi tạo và xóa các Pod. Mỗi Pod đươc khởi tạo sẽ có một địa chỉ IP riêng, tuy nhiên IP này sẽ không được gọi là stable và bắt buộc vào các Pod. Điều này đặt ra câu hỏi, ví dụ trong trường hợp cả cluster chỉ có front và backend được chia làm 2 group, thì làm sao 2 group này có thể nói chuyện với nhau. làm sau chúng có thể discovery và giữ trạng thái kết nối ổn định.

## on `Service`

- K8S cho phép logic nhiều Pod lại thành một group, cho phép truy cập vào group này một cách thông minh, được hiểu như đang môi trường microservice. Các nhóm Pod này được gọi là Service, và được truy cập qua các Selector.
- Ví dụ có một image thực hiện việc xử lý backend và có 3 nhân bản. Những bản copy là việc xử lý nội bộ, front hoàn toàn sẽ không biết việc các backend đang được nhân bản.
- Các ứng dụng trong K8S sẽ được cung cấp các Endpoint API, Service cung cấp một group các Pod, khi Pod thay đổi thì service thay đổi


## Define Service

- Khởi tạo một Service có selector.  Service sẽ được gắn cho một IP hay còn được gọi là  Cluster IP, trên cổng 80/TCP sẽ được forword sang cổng 9376 trên Pod có lable là app=myapp
```
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```


- Service ánh xạ giúp truy cập vào Pod, tuy nhiên cũng có thể ánh xạ các loại backend khác ví dụ như


## VIP and Service Agent

- Trong K8S cluster, mỗi node đều có một kube-proxy, các tiến trình này đảm nhiệm cung cấp VIP cho các Service

## userspace proxy mode

- This model, kube-proxy monitors Kubernetes master of Serviceobjects and Endpointsto add and remove objects. For each Service, it will open a port on the local Node (randomly selected). Any requests a connection to "proxy port," and will be proxied to Servicethe backend Podsa above (e.g., Endpointsas reported). Use which backend Pod, based on Servicethe SessionAffinitydetermined. Finally, it installs iptables rules to capture the arrival Serviceof clusterIP(virtual IP) and Portrequest and redirected to the proxy port, proxy port and then proxy requests to backend Pod. Results of the network returns, any arrival Serviceof IP: Port's request, will be the agent to a suitable backend, the client does not need to know about Kubernetes, Service, or Podany information. The default strategy is to select backend through the round-robin algorithm Pod. Based on the client IP session affinity, by setting service.spec.sessionAffinitythe value of "ClientIP"(the default value "None").

## iptables proxy mode
- This model, kube-proxy monitors Kubernetes master of Serviceobjects and Endpointsto add and remove objects. For each Service, it will install iptables rules to capture the arrival Serviceof the clusterIP(virtual IP) and a port request, and then redirects the request to Servicean upper set of backend. For each Endpointsobject, it will install iptables rules, this rule would choose a backend Pod. The default strategy is to randomly choose a backend. Based on the client IP session affinity, you can service.spec.sessionAffinityset the value "ClientIP"(the default value "None"). And userspace similar agency, the network returns the result, any arrival Serviceof IP: Port's request, will be the agent to a suitable backend, the client does not need to know about Kubernetes, Service, or Podany information. This should be faster and more reliable than the userspace proxy. However, unlike userspace agent, if the initially selected Poddoes not respond, the proxy does not retry iptables another automatically Pod, so it relies Readiness Probes .

## ipvs proxy mode
- In this mode, kube-proxy will monitor Kubernetes Serviceobjects and Endpointscall netlinkinterfaces to create ipvs rules accordingly and periodically synchronize ipvs rules with Kubernetes Serviceobjects and Endpointsobjects to ensure that the ipvs state is consistent with expectations. When accessing the service, the traffic will be redirected to one of the backend Pods.

- Similar to iptables, ipvs is based on the hook function of netfilter, but uses a hash table as the underlying data structure and works in kernel space. This means that ipvs can redirect traffic faster and have better performance when synchronizing proxy rules. In addition, ipvs provides more options for load balancing algorithms, such as:

    - rr: Round-robin scheduling
    - lc: Minimum number of connections
    - dh: Target hash
    - sh: Source hash
    - sed: Minimum expected delay
    - nq: No queue scheduling

## Why not use round-robin DNS?
- A question that arises from time to time is why we all use the VIP method instead of the standard round-robin DNS for several reasons:
    - For a long time, the DNS database failed to take DNS TTL and cache domain name query results seriously
    - Many applications only query DNS once and cache the results
    - Even if the application and library can query and parse correctly, the load caused by repeated re-parse of each client is very difficult to manage
    - We try our best to prevent users from doing things that are not good for them. If many people come to ask this question, we may choose to implement it.

## Service discovery

-  Có 2 cách cơ bản để link các service với nhau; dựa vào biến môi trường hoặc DNS
- Environment variables

- DNS ( recommen )


## Publishing Service-Service Type

- Có một số phương pháp để public các service như sau: 
    - ClusterIP:  expose Service thông qua địa chỉ IP trong cluster
    - Nodeport: expose qua địa chỉ IP trên các node. Bên ngoài sẽ access vào service dưới dạng <NodeIP>:<NodePort>NodePort
    - LoadBalancer: Use the cloud provider's load balancer to expose services to the outside world. External load balancer can be routed to NodePortservices and ClusterIPservice.
    - ExternalName: By returning CNAMEand its value, the service may be mapped to the externalNamecontents of the field (e.g., foo.bar.example.com). Without any type of proxy is created, which can only Kubernetes version 1.7 or later kube-dnsis supported.