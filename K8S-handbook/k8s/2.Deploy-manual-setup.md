## Kubernetes The Hard Way



## 1. Cài đặt Client Tool 

Cài đặt cfssl  và cfssljson để khởi tạo PKI và TLS Certificate
```
yum install wget -y
wget -q --timestamping \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/linux/cfssl \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/linux/cfssljson
chmod +x cfssl cfssljson
mv cfssl cfssljson /usr/local/bin/

```

Kiểm tra version 
```
cfssl version
cfssljson --version

```

Cài đặt Kubectl v18.0
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

```

Kiểm tra version Kubectl
```
kubectl version --client

```


## 2. Khởi tạo CA và tạo TLS Certificate

Trong bài Lab này,  cfssl sẽ được sử dụng để khởi tạo PKI, và sử dụng nó để boostrap một Certificate Authority ( CA ), tiếp đó sẽ tạo một TLS cerficiate cho các thành phần: etcd, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet và kubeproxy

### Khởi tạo CA config file, certificate và private key
```
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "MeditechJSC",
      "ST": "System"
    }
  ]
}
EOF

cfssl gencert -config ca-config.json -initca ca-csr.json | cfssljson -bare ca

```

Kết quả sẽ tạo ra 2 file
```
ca-key.pem
ca.pem
```

### Từ CA public key và private key đã được tạo ở trên, thực hiện khởi tạo các Client Certificate cho các dịch vụ
### Khởi tạo "Admin" client certificate và private key
```

cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Sytstem"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

```

Kết quả sẽ tạo 2 file
```
admin-key.pem
admin.pem

```

### Khởi tạo certificate cho các từng worker node, được sử dụng bởi kubelet. Trong bài lab này có 2 node thì sẽ tạo ra 2 certificate 
```
cat > worker1-csr.json <<EOF
{
  "CN": "system:node:worker1",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Node"
    }
  ]
}
EOF

cat > worker2-csr.json <<EOF
{
  "CN": "system:node:worker2",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Node"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=worker1 \
  -profile=kubernetes \
   worker1-csr.json | cfssljson -bare worker1

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=worker2 \
  -profile=kubernetes \
   worker2-csr.json | cfssljson -bare worker2


```

Kết quả sẽ tạo ra  4 file
```
worker1-key.pem
worker1.pem
worker2-key.pem
worker2.pem
```

### Khởi tạo Client Certificate cho kube-controller-manager
```
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "Vi",
      "L": "Hanoi",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "manager"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager


```

Kết quả sẽ tạo ra 2 key
```
kube-controller-manager-key.pem
kube-controller-manager.pem
```

### Khởi tạo Client Certificate cho Kube-Proxy

```

cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "KubeProxy"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy


```

Kết quả sẽ tạo ra 2 file
```
kube-proxy-key.pem
kube-proxy.pem
```

### Khởi tạo Client Certificate cho Scheduler

```
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HANOI",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Scheduler"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler

```

Kết quả sẽ tạo ra 2 file
```
kube-scheduler-key.pem
kube-scheduler.pem
```

### Khởi tạo Client Certificate cho APIServer


```
KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Kube"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=master1,master2,master3,10.10.203.29,10.10.203.31,10.10.203.30,10.10.203.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```

Kết quả sẽ tạo ra 2 file
```
kubernetes-key.pem
kubernetes.pem
```

### Khởi tạo Client Certificate cho Server Account

```
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Service"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```



Kết quả sẽ trả về 2 file
```
service-account-key.pem
service-account.pem
```


### Khởi tạo Certificate cho Calio
```
cat > calio-csr.json <<EOF
{
  "CN": "calio",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Calio"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  calio-csr.json | cfssljson -bare calio
```

Kết quả sẽ tạo ra 2 file
```
calio-key.pem
calio.pem

```

### Copy Client và Server Certificate tới các node Worker

```
for instance in worker1 worker2; do
  scp  kubernetes-key.pem kubernetes.pem ca.pem calio.pem calio-key.pem ${instance}-key.pem ${instance}.pem ${instance}:~/
done
```

### Copy Client va Server Certificate tới các node Master 
```
for instance in master1 master2 master3; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem ${instance}:~/
done
```



## 3. Cấu hình TLS Certificate cho các service

### Khởi tạo Kubeconfig
- Thực hiện khởi tạo kubeconfig cho các máy worker ( kubelet )
```
IP_VIP=10.10.203.29

for instance in worker1 worker2; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${IP_VIP}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig 
done
```

Kết quả sẽ trả về 2 file
```
worker1.kubeconfig
worker2.kubeconfig

```

### Khởi tạo cấu hình cho kube-proxy

```
IP_VIP=10.10.203.29

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${IP_VIP}:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

Kết quả sẽ tạo ra file
```
kube-proxy.kubeconfig

```

### Khởi tạo cấu hình cho kube-controller-manager

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```

Kết quả sẽ tạo ra file
```
kube-controller-manager.kubeconfig

```

### Khởi tạo cấu hình cho kube scheduler

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```

Kết quả sẽ tạo ra
```
kube-scheduler.kubeconfig

```


### Khởi tạo cấu hình cho Admin

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```

Kết quả sẽ tạo ra file
```
admin.kubeconfig

```

### Khởi tạo cấu hình cho Calio.

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=cni.kubeconfig

kubectl config set-credentials calico-cni \
  --client-certificate=calio.pem \
  --client-key=calio-key.pem \
  --embed-certs=true \
  --kubeconfig=cni.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=calico-cni \
  --kubeconfig=cni.kubeconfig

kubectl config use-context default --kubeconfig=cni.kubeconfig
```


### Copy cấu hình kubelet, cni-calio và kubeproxy tới các node Worker
```
for instance in worker1 worker2; do
  scp ${instance}.kubeconfig cni.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done
```

### Copy cấu hình kube-controller-manager và kube-scheduler lên các node Master
```
for instance in master1 master2 master3; do
  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done
```



## 4. Khởi tạo  Data Encryption Key

Kube hỗ trợ khả năng mã hóa các cấu hình, trạng thái trong cluster. 
Khởi tạo encrypt key
```
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

```

Khởi tạo Object
```
cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

- Sao chép sang các máy chủ master
```
for instance in master1 master2 master3; do
  scp encryption-config.yaml ${instance}:~/
done
```

## 5. Khởi tạo Etcd Cluster

- Truy cập vào máy chủ master1, master2, master3
```
ssh master1
ssh master2
ssh master3

```

Trên máy chủ này thực hiện tải về Etcd từ Github 
```
wget -q --timestamping \
  "https://github.com/etcd-io/etcd/releases/download/v3.4.0/etcd-v3.4.0-linux-amd64.tar.gz"
```


Giải nén Etcd, và di chuyển vào thư mục binary
```
tar -xvf etcd-v3.4.0-linux-amd64.tar.gz
mv etcd-v3.4.0-linux-amd64/etcd* /usr/local/bin/
```

Khởi tạo thư mục chứa dữ liệu, sao chép TLS Certificate
```
mkdir -p /etc/etcd /var/lib/etcd
cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
```

Khởi tạo Service Etcd. Sử dụng INTERNAL_IP chính là địa chỉ đường Manage của node này, cũng là đường liên hệ giữa cluster
Ngoài ra tại option initial-cluster, thực hiện chỉnh sửa hostname và địa chỉ IP manage của các node master còn lại
```

ETCD_NAME=$(hostname -s)

## chỉnh sửa IP với từng node tương ứng
INTERNAL_IP=10.10.203.30
INTERNAL_IP=10.10.203.31
INTERNAL_IP=10.10.203.52


cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master1=https://10.10.203.31:2380,master2=https://10.10.203.30:2380,master3=https://10.10.203.52:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

Khởi động service Etcd. Lưu ý sẽ start đồng thời serivce trên cả 3 node để thực hiện peering cluster.
```
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
```

Kiểm tra trạng thái Etcd
```
[root@master1 key]#  systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2020-04-09 11:16:09 +07; 13s ago
     Docs: https://github.com/coreos
 Main PID: 16772 (etcd)
   CGroup: /system.slice/etcd.service
           └─16772 /usr/local/bin/etcd --name master1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kube...

Apr 09 11:16:09 master1 etcd[16772]: a1e038d56b9d9de9 initialized peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream Message writer)
Apr 09 11:16:09 master1 etcd[16772]: ready to serve client requests
Apr 09 11:16:09 master1 etcd[16772]: ready to serve client requests
Apr 09 11:16:09 master1 etcd[16772]: published {Name:master1 ClientURLs:[https://10.10.203.31:2379]} to cluster 3239a6a4243cf8e5
Apr 09 11:16:09 master1 systemd[1]: Started etcd.
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream Message reader)
Apr 09 11:16:09 master1 etcd[16772]: serving client requests on 10.10.203.31:2379
Apr 09 11:16:09 master1 etcd[16772]: serving client requests on 127.0.0.1:2379
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream MsgApp v2 reader)

```

Kiểm tra danh sách member có trong cluster Etcd
```

[root@master3 ~]# ETCDCTL_API=3 etcdctl -w table endpoint --cluster status    --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.pem   --cert=/etc/etcd/kubernetes.pem   --key=/etc/etcd/kubernetes-key.pem
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://10.10.203.30:2379 | 128a267aefa911c8 |   3.4.0 |  5.9 MB |      true |      false |       360 |       2454 |               2454 |        |
| https://10.10.203.31:2379 | a1e038d56b9d9de9 |   3.4.0 |  5.9 MB |     false |      false |       360 |       2454 |               2454 |        |
| https://10.10.203.52:2379 | f90331b1cff48a13 |   3.4.0 |  5.9 MB |     false |      false |       360 |       2454 |               2454 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

ETCDCTL_API=3 etcdctl -w json endpoint status   --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.pem   --cert=/etc/etcd/kubernetes.pem   --key=/etc/etcd/kubernetes-key.pem

```


## 6. Khởi tạo Kubernetes Control Plane

Danh sách các thành phần trong Control Plane: Kubernetes API Server, Scheduler, and Controller Manager.
Thực hiện trên 3 máy chủ master
```
ssh master1
ssh master2
ssh master3
```


Khởi tạo thư mục chứa file cấu hình
```
mkdir -p /etc/kubernetes/config
```

Tải về binary của các thành phần cần cài đặt
```
wget -q  \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl"
```

Thực hiện phân quyền và copy vào thư mục binary của hệ thống
```
chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
```

###  Cấu hình API Server

```
mkdir -p /var/lib/kubernetes/

mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
  service-account-key.pem service-account.pem \
  encryption-config.yaml /var/lib/kubernetes/
```

Tương ứng với từng máy chủ master, địa chỉ INTERNAI_IP sẽ khác. Ngoài ra cần chú ý thêm địa chỉ của etcd-server chính là địa chỉ các etcd server vừa tạo. service-cluster-ip-range sẽ là địa chỉ của Cluster IP ( sẽ tìm hiểu trong tương lai, hiện tại chỉ cần chọn subnet không cần trùng với mạng vật lý )
```
INTERNAL_IP=10.10.203.30
INTERNAL_IP=10.10.203.31
INTERNAL_IP=10.10.203.52 

```

Cấu hình Service
```
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```


###  Cấu hình Kube Controller Manager

Sao chép cấu hình kubemanager
```
mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

Khởi tạo kube-controller-manager.service. Chú ý : cluster-cidr sẽ là subnet mà các Pod endpoint được gắn
```
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```


### Cấu hình Configure the Kubernetes Scheduler

Chuyển cấu hình kube-scheduler
```
mv kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Khởi tạo cấu hình kube-scheduler.yaml

```
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF
```

khởi tạo scheduler service
```
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### Khởi động các dịch vụ

Khởi động các dịch trên controller
```
systemctl daemon-reload
systemctl enable kube-apiserver kube-controller-manager kube-scheduler
systemctl start kube-apiserver kube-controller-manager kube-scheduler
systemctl restart kube-apiserver kube-controller-manager kube-scheduler
systemctl status kube-apiserver kube-controller-manager kube-scheduler

```


### Kiểm tra dịch vụ

Kiểm tra dịch vụ đã xanh lè xanh lét thì tiếp tục làm các bước tiếp theo
Có thể trạng thái đã OK nhưng vẫn xuất hiện lỗi trong quá trình vận hành
Quá trình boostrap và peering giữa các service khá lâu, và xuất hiện lỗi có thể phải chờ 5 - 10 phút để hệ thống hoạt động  ổ định.


### Kiểm tra trạng thái
```
# kubectl get componentstatuses --kubeconfig admin.kubeconfig
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}
[root@master1 ~]#

```


### Câu hình RBAC cho Kubelet
Cấu hình RBAC cho phép  Kubernetes API Server access vào Kubelet API trên các worker để lấy metric, log và chạy command
```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF
```

Bind system:kube-apiserver-to-kubeletClusterRole  cho User kubernetes 
```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```

### Cấu hình RBAC cho  CNI plugin - Calio để truy cập vào Kubenetes
```
kubectl apply --kubeconfig admin.kubeconfig -f - <<EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-cni
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  # The CNI plugin patches pods/status.
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
 # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - ipamconfigs
      - clusterinformations
      - ippools
    verbs:
      - get
      - list
EOF
```

Binding Role
```
kubectl create clusterrolebinding calico-cni --clusterrole=calico-cni --user=calico-cni
```

## 6. Khởi tạo Kube Worker node

### Cài đặt các gói yêu cầu, Runtime env

Cài gói yêu cầu
```
yum install -y  socat conntrack ipset
```

Kube sẽ bị lỗi khi sử dụng swap, disable swap trên Worker
```
swapon --show 
swapoff -a
 
```

Cài đặt Docker để làm Runtime Env. Có thể lựa chọn các giải pháp khác
```bash
# Install Docker CE
## Set up the repository
### Install required packages.
yum install -y yum-utils device-mapper-persistent-data lvm2

### Add Docker repository.
yum-config-manager --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo

## Install Docker CE.
yum update -y && yum install -y \
  containerd.io-1.2.13 \
  docker-ce-19.03.8 \
  docker-ce-cli-19.03.8

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker

```

- Kiểm tra cgroup

```
docker info | grep -i cgroup
 Cgroup Driver: systemd

```


Tải về và cài đặt các binary cần thiết
```
wget -q \
https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubelet \
  https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz
```



Khởi tạo thư mục
```
sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```


Cài đặt các binary
```
tar -xvf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/bin/
chmod +x  kubectl kube-proxy kubelet  
mv kubectl kube-proxy kubelet /usr/local/bin/
```



### Cấu hình Kubelet
```
mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
mv ca.pem /var/lib/kubernetes/
```


Khởi tạo kubelet-config.yaml
Trong đó clusterDNS chỉnh địa chỉ của Cluster IP ( service-cluster-ip-range ) của CoreDNS sẽ khởi tạo sau.
POD_CIDR chính là subnet cluster-cidr ( CIDR Range for Pods in cluster) : 10.200.0.0/16 

POD_CIDR=10.200.0.0/16 

```
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
```

Cấu hình file service cho kubelet
```
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=5m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --cgroup-driver=systemd \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### Cấu hình Kube-proxy

Sao chép cấu hình
```
mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
```

Khởi tạo Object
```

POD_CIDR=10.200.0.0/16 

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "${POD_CIDR}"
EOF
```

Khởi tạo cấu hình systemd
```
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

```
### Khởi động Kubelet và Kube-proxy
```
systemctl daemon-reload
systemctl enable docker kubelet kube-proxy
systemctl stop docker kubelet kube-proxy
systemctl start docker kubelet kube-proxy

systemctl status docker kubelet kube-proxy

```

### Xem danh sách các node
```
[root@master1 ~]# kubectl get nodes --kubeconfig admin.kubeconfig
NAME      STATUS   ROLES    AGE   VERSION
worker1   Ready    <none>   11m   v1.18.0
worker2   Ready    <none>   30s   v1.18.0

```
### Cài đặt Calio
The calico/node docker container must be run on the Kubernetes master and each Kubernetes node in your cluster. It contains the BGP agent necessary for Calico routing to occur, and the Felix agent which programs network policy rules.

The cni-plugin plugin integrates directly with the Kubernetes kubelet process on each node to discover which pods have been created, and adds them to Calico networking.

The calico/kube-controllers container runs as a pod on top of Kubernetes and keeps Calico in-sync with Kubernetes.

### Thực hiện cấu hình CNI trên các máy Worker



Sao chép TLS Certificate của Etcd
```
cp kubernetes-key.pem kubernetes.pem /var/lib/kubernetes
```


Khởi tạo file service. chỉnh sửa IP và Nodename để phù hợp
```

vi /etc/systemd/system/calico-node.service
[Unit]
Description=calico-node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=  \
  -e ETCD_ENDPOINTS=https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379 \
  -e ETCD_CA_CERT_FILE=/var/lib/kubernetes/ca.pem \
  -e ETCD_CERT_FILE=/var/lib/kubernetes/kubernetes.pem \
  -e ETCD_KEY_FILE=/var/lib/kubernetes/kubernetes-key.pem \
  -e NODENAME=worker2 \
  -e IP=10.10.203.33 \
  -e IP6= \
  -e AS=4890 \
  -e CALICO_IPV4POOL_CIDR=10.200.0.0/16 \
  -e CALICO_NETWORKING_BACKEND=bird \
  -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
  -v /lib/modules:/lib/modules \
  -v /run/docker/plugins:/run/docker/plugins \
  -v /var/run/calico:/var/run/calico \
  -v /var/log/calico:/var/log/calico \
  -v /var/lib/calico:/var/lib/calico \
  -v /var/lib/kubernetes:/var/lib/kubernetes \
  calico/node:v3.8.4
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Khởi động Service
```
systemctl daemon-reload
systemctl stop calico-node

systemctl status calico-node
```

Log
```
Apr 10 10:35:44 worker2 systemd: Started libcontainer container 97aa83a19a475227a55dc659a047217859106170b99c7cd9cf3213644220df41.
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 256: Early log level set to info
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 272: Using NODENAME environment for node name
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 284: Determined node name: worker2
Apr 10 10:35:46 worker2 docker: 2020-04-10 03:35:46.206 [INFO][8] startup.go 97: Skipping datastore connection test
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.006 [INFO][8] startup.go 367: Building new node resource Name="worker2"
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.007 [INFO][8] startup.go 382: Initialize BGP data
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.007 [INFO][8] startup.go 476: Using IPv4 address from environment: IP=10.10.203.33
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.008 [INFO][8] startup.go 509: IPv4 address 10.10.203.33 discovered on interface eth0
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.008 [INFO][8] startup.go 452: Node IPv4 changed, will check for conflicts
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.162 [INFO][8] startup.go 642: Using AS number specified in environment (AS=4890)
Apr 10 10:35:48 worker2 docker: 2020-04-10 03:35:48.006 [INFO][8] startup.go 181: Using node name: worker2
Apr 10 10:35:49 worker2 docker: Calico node started successfully
Apr 10 10:35:54 worker2 kernel: ip6_tables: (C) 2000-2006 Netfilter Core Team

```



Cài đặt Calio Plugin
```
curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/v3.8.4/calico-amd64
chmod 755 /opt/cni/bin/calico


curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/v3.8.4/calico-ipam-amd64
chmod 755 /opt/cni/bin/calico-ipam
```

Khởi tạo thưc mục cấu hình
```
mkdir -p /etc/cni/net.d/
```

Copy cấu hình CNI-Calio
```
cp cni.kubeconfig  /var/lib/kubernetes/
chmod 644 /var/lib/kubernetes/cni.kubeconfig
```

Khởi tạo cấu hình CNI Plugin
```
cat > /etc/cni/net.d/10-calico.conf <<EOF
{
    "name": "calico-k8s-network",
    "cniVersion": "0.3.1",
    "type": "calico",
    "etcd_endpoints": "https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379",
    "etcd_key_file": "/var/lib/kubernetes/kubernetes-key.pem",
    "etcd_cert_file": "/var/lib/kubernetes/kubernetes.pem",
    "etcd_ca_cert_file": "/var/lib/kubernetes/ca.pem",
    "log_level": "info",
    "ipam": {
        "type": "calico-ipam"
    },
    "policy": {
        "type": "k8s"
    },
    "kubernetes": {
        "kubeconfig": "/var/lib/kubernetes/cni.kubeconfig"
    }
}
EOF

cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.3.1",
    "name": "lo",
    "type": "loopback"
}
EOF
```


### Thực hiện cấu hình CNI trên các máy Master


Khởi tạo object file 
```
cat <<EOF> calico-kube-controllers.yaml
# Calico Version v3.8.4
# https://docs.projectcalico.org/v3.4/releases#v3.8.4
# This manifest includes the following component versions:
#   calico/kube-controllers:v3.8.4

# Create this manifest using kubectl to deploy
# the Calico Kubernetes controllers.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
spec:
  # Only a single instance of the this pod should be
  # active at a time.  Since this pod is run as a Deployment,
  # Kubernetes will ensure the pod is recreated in case of failure,
  # removing the need for passive backups.
  replicas: 2
  selector:
    matchLabels:
      k8s-app: calico-kube-controllers
  strategy:
    type: Recreate
  template:
    metadata:
      name: calico-kube-controllers
      namespace: kube-system
      labels:
        k8s-app: calico-kube-controllers
    spec:
      hostNetwork: true
      containers:
        - name: calico-kube-controllers
          # Make sure to pin this to your desired version.
          image: calico/kube-controllers:v3.8.4
          env:
            # Configure the location of your etcd cluster.
            - name: ETCD_ENDPOINTS
              value: "https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379"
            - name: ETCD_CA_CERT_FILE	
              value: "/var/lib/kubernetes/ca.pem"
            - name: ETCD_CERT_FILE	
              value: "/var/lib/kubernetes/kubernetes.pem"
            - name: ETCD_KEY_FILE	
              value: "/var/lib/kubernetes/kubernetes-key.pem" 
          volumeMounts:
            - name: etcd-tls
              mountPath: /var/lib/kubernetes
      volumes:
        - name: etcd-tls
          hostPath:
            # directory location on host
            path: /var/lib/kubernetes
            # this field is optional
            type: Directory
EOF
```


Khởi  tạo Pod Controller
```
kubectl create -f calico-kube-controllers.yaml
kubectl get pods --namespace=kube-system

NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-fc67c99bf-2l25b   1/1     Running   0          14m
calico-kube-controllers-fc67c99bf-vmljz   1/1     Running   0          14m

```

Cấu hình RBAC
```
 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/rbac/rbac-kdd-calico.yaml
 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/rbac/rbac-etcd-calico.yaml

```



### Cài đặt Calio
The calico/node docker container must be run on the Kubernetes master and each Kubernetes node in your cluster. It contains the BGP agent necessary for Calico routing to occur, and the Felix agent which programs network policy rules.

The cni-plugin plugin integrates directly with the Kubernetes kubelet process on each node to discover which pods have been created, and adds them to Calico networking.

The calico/kube-controllers container runs as a pod on top of Kubernetes and keeps Calico in-sync with Kubernetes.

#### Thực hiên cấu hình trên máy các máy Master
Cài gói yêu cầu
```
yum install -y  socat conntrack ipset
```

Kube sẽ bị lỗi khi sử dụng swap, disable swap trên Worker
```
swapon --show 
swapoff -a
 
```

Cài đặt Docker để làm Runtime Env. Có thể lựa chọn các giải pháp khác
```bash
# Install Docker CE
## Set up the repository
### Install required packages.
yum install -y yum-utils device-mapper-persistent-data lvm2

### Add Docker repository.
yum-config-manager --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo

## Install Docker CE.
yum update -y && yum install -y \
  containerd.io-1.2.13 \
  docker-ce-19.03.8 \
  docker-ce-cli-19.03.8

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker

```

- Kiểm tra cgroup

```
docker info | grep -i cgroup
 Cgroup Driver: systemd

```



### Thực hiện cấu hình CNI trên các máy Worker



Sao chép TLS Certificate của Etcd
```
cp kubernetes-key.pem kubernetes.pem /var/lib/kubernetes
```


Khởi tạo file service. chỉnh sửa IP và Nodename để phù hợp
```

vi /etc/systemd/system/calico-node.service
[Unit]
Description=calico-node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=  \
  -e ETCD_ENDPOINTS=https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379 \
  -e ETCD_CA_CERT_FILE=/var/lib/kubernetes/ca.pem \
  -e ETCD_CERT_FILE=/var/lib/kubernetes/kubernetes.pem \
  -e ETCD_KEY_FILE=/var/lib/kubernetes/kubernetes-key.pem \
  -e NODENAME=worker2 \
  -e IP=10.10.203.33 \
  -e IP6= \
  -e AS=4890 \
  -e CALICO_IPV4POOL_CIDR=10.200.0.0/16 \
  -e CALICO_NETWORKING_BACKEND=bird \
  -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
  -v /lib/modules:/lib/modules \
  -v /run/docker/plugins:/run/docker/plugins \
  -v /var/run/calico:/var/run/calico \
  -v /var/log/calico:/var/log/calico \
  -v /var/lib/calico:/var/lib/calico \
  -v /var/lib/kubernetes:/var/lib/kubernetes \
  calico/node:v3.8.4
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Khởi động Service
```
systemctl daemon-reload
systemctl stop calico-node

systemctl status calico-node
```

Log
```
Apr 10 10:35:44 worker2 systemd: Started libcontainer container 97aa83a19a475227a55dc659a047217859106170b99c7cd9cf3213644220df41.
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 256: Early log level set to info
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 272: Using NODENAME environment for node name
Apr 10 10:35:45 worker2 docker: 2020-04-10 03:35:45.808 [INFO][8] startup.go 284: Determined node name: worker2
Apr 10 10:35:46 worker2 docker: 2020-04-10 03:35:46.206 [INFO][8] startup.go 97: Skipping datastore connection test
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.006 [INFO][8] startup.go 367: Building new node resource Name="worker2"
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.007 [INFO][8] startup.go 382: Initialize BGP data
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.007 [INFO][8] startup.go 476: Using IPv4 address from environment: IP=10.10.203.33
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.008 [INFO][8] startup.go 509: IPv4 address 10.10.203.33 discovered on interface eth0
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.008 [INFO][8] startup.go 452: Node IPv4 changed, will check for conflicts
Apr 10 10:35:47 worker2 docker: 2020-04-10 03:35:47.162 [INFO][8] startup.go 642: Using AS number specified in environment (AS=4890)
Apr 10 10:35:48 worker2 docker: 2020-04-10 03:35:48.006 [INFO][8] startup.go 181: Using node name: worker2
Apr 10 10:35:49 worker2 docker: Calico node started successfully
Apr 10 10:35:54 worker2 kernel: ip6_tables: (C) 2000-2006 Netfilter Core Team

```



Cài đặt Calio Plugin
```
curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/v3.8.4/calico-amd64
chmod 755 /opt/cni/bin/calico


curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/v3.8.4/calico-ipam-amd64
chmod 755 /opt/cni/bin/calico-ipam
```

Khởi tạo thưc mục cấu hình
```
mkdir -p /etc/cni/net.d/
```

Copy cấu hình CNI-Calio
```
cp cni.kubeconfig  /var/lib/kubernetes/
chmod 644 /var/lib/kubernetes/cni.kubeconfig
```

Khởi tạo cấu hình CNI Plugin
```
cat > /etc/cni/net.d/10-calico.conf <<EOF
{
    "name": "calico-k8s-network",
    "cniVersion": "0.3.1",
    "type": "calico",
    "etcd_endpoints": "https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379",
    "etcd_key_file": "/var/lib/kubernetes/kubernetes-key.pem",
    "etcd_cert_file": "/var/lib/kubernetes/kubernetes.pem",
    "etcd_ca_cert_file": "/var/lib/kubernetes/ca.pem",
    "log_level": "info",
    "ipam": {
        "type": "calico-ipam"
    },
    "policy": {
        "type": "k8s"
    },
    "kubernetes": {
        "kubeconfig": "/var/lib/kubernetes/cni.kubeconfig"
    }
}
EOF

cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.3.1",
    "name": "lo",
    "type": "loopback"
}
EOF
```


### Thực hiện cấu hình CNI trên các máy Master


Khởi tạo object file 
```
cat <<EOF> calico-kube-controllers.yaml
# Calico Version v3.8.4
# https://docs.projectcalico.org/v3.4/releases#v3.8.4
# This manifest includes the following component versions:
#   calico/kube-controllers:v3.8.4

# Create this manifest using kubectl to deploy
# the Calico Kubernetes controllers.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
spec:
  # Only a single instance of the this pod should be
  # active at a time.  Since this pod is run as a Deployment,
  # Kubernetes will ensure the pod is recreated in case of failure,
  # removing the need for passive backups.
  replicas: 2
  selector:
    matchLabels:
      k8s-app: calico-kube-controllers
  strategy:
    type: Recreate
  template:
    metadata:
      name: calico-kube-controllers
      namespace: kube-system
      labels:
        k8s-app: calico-kube-controllers
    spec:
      hostNetwork: true
      containers:
        - name: calico-kube-controllers
          # Make sure to pin this to your desired version.
          image: calico/kube-controllers:v3.8.4
          env:
            # Configure the location of your etcd cluster.
            - name: ETCD_ENDPOINTS
              value: "https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379"
            - name: ETCD_CA_CERT_FILE	
              value: "/var/lib/kubernetes/ca.pem"
            - name: ETCD_CERT_FILE	
              value: "/var/lib/kubernetes/kubernetes.pem"
            - name: ETCD_KEY_FILE	
              value: "/var/lib/kubernetes/kubernetes-key.pem" 
          volumeMounts:
            - name: etcd-tls
              mountPath: /var/lib/kubernetes
      volumes:
        - name: etcd-tls
          hostPath:
            # directory location on host
            path: /var/lib/kubernetes
            # this field is optional
            type: Directory
EOF
```


Khởi  tạo Pod Controller
```
kubectl create -f calico-kube-controllers.yaml
kubectl get pods --namespace=kube-system

NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-fc67c99bf-2l25b   1/1     Running   0          14m
calico-kube-controllers-fc67c99bf-vmljz   1/1     Running   0          14m

```

Cấu hình RBAC
```
 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/rbac/rbac-kdd-calico.yaml
 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/rbac/rbac-etcd-calico.yaml

```
