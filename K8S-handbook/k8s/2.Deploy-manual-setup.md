## Kubernetes The Hard Way



## 1. Cài đặt Client Tool 

Cài đặt cfssl  và cfssljson để khởi tạo PKI và TLS Certificate
```
yum install wget -y
wget -q --timestamping \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/linux/cfssl \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/linux/cfssljson
chmod +x cfssl cfssljson
mv cfssl cfssljson /usr/local/bin/

```

Kiểm tra version 
```
cfssl version
cfssljson --version

```

Cài đặt Kubectl v18.0
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

```

Kiểm tra version Kubectl
```
kubectl version --client

```


## 2. Khởi tạo CA và tạo TLS Certificate

Trong bài Lab này,  cfssl sẽ được sử dụng để khởi tạo PKI, và sử dụng nó để boostrap một Certificate Authority ( CA ), tiếp đó sẽ tạo một TLS cerficiate cho các thành phần: etcd, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet và kubeproxy

### Khởi tạo CA config file, certificate và private key
```
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "MeditechJSC",
      "ST": "System"
    }
  ]
}
EOF

cfssl gencert -config ca-config.json -initca ca-csr.json | cfssljson -bare ca

```

Kết quả sẽ tạo ra 2 file
```
ca-key.pem
ca.pem
```

### Từ CA public key và private key đã được tạo ở trên, thực hiện khởi tạo các Client Certificate cho các dịch vụ
### Khởi tạo "Admin" client certificate và private key
```

cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Sytstem"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

```

Kết quả sẽ tạo 2 file
```
admin-key.pem
admin.pem

```

### Khởi tạo certificate cho các từng worker node, được sử dụng bởi kubelet. Trong bài lab này có 2 node thì sẽ tạo ra 2 certificate 
```
cat > worker1-csr.json <<EOF
{
  "CN": "system:node:worker1",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Node"
    }
  ]
}
EOF

cat > worker2-csr.json <<EOF
{
  "CN": "system:node:worker2",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Node"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=worker1 \
  -profile=kubernetes \
   worker1-csr.json | cfssljson -bare worker1

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=worker2 \
  -profile=kubernetes \
   worker2-csr.json | cfssljson -bare worker2


```

Kết quả sẽ tạo ra  4 file
```
worker1-key.pem
worker1.pem
worker2-key.pem
worker2.pem
```

### Khởi tạo Client Certificate cho kube-controller-manager
```
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "Vi",
      "L": "Hanoi",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "manager"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager


```

Kết quả sẽ tạo ra 2 key
```
kube-controller-manager-key.pem
kube-controller-manager.pem
```

### Khởi tạo Client Certificate cho Kube-Proxy

```

cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "KubeProxy"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy


```

Kết quả sẽ tạo ra 2 file
```
kube-proxy-key.pem
kube-proxy.pem
```

### Khởi tạo Client Certificate cho Scheduler

```
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HANOI",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Scheduler"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler

```

Kết quả sẽ tạo ra 2 file
```
kube-scheduler-key.pem
kube-scheduler.pem
```

### Khởi tạo Client Certificate cho APIServer


```
KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Kube"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=master1,master2,master3,10.10.203.29,10.10.203.31,10.10.203.30,10.10.203.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```

Kết quả sẽ tạo ra 2 file
```
kubernetes-key.pem
kubernetes.pem
```

### Khởi tạo Client Certificate cho Server Account

```
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Service"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```



Kết quả sẽ trả về 2 file
```
service-account-key.pem
service-account.pem
```


### Khởi tạo Certificate cho Calio
```
cat > calio-csr.json <<EOF
{
  "CN": "calio",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HaNoi",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Calio"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  calio-csr.json | cfssljson -bare calio
```

Kết quả sẽ tạo ra 2 file
```
calio-key.pem
calio.pem

```

### Copy Client và Server Certificate tới các node Worker

```
for instance in worker1 worker2; do
  scp ca.pem calio.pem calio-key.pem ${instance}-key.pem ${instance}.pem ${instance}:~/
done
```

### Copy Client va Server Certificate tới các node Master 
```
for instance in master1 master2 master3; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem ${instance}:~/
done
```



## 3. Cấu hình TLS Certificate cho các service

### Khởi tạo Kubeconfig
- Thực hiện khởi tạo kubeconfig cho các máy worker ( kubelet )
```
IP_VIP=10.10.203.29

for instance in worker1 worker2; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${IP_VIP}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig 
done
```

Kết quả sẽ trả về 2 file
```
worker1.kubeconfig
worker2.kubeconfig

```

### Khởi tạo cấu hình cho kube-proxy

```
IP_VIP=10.10.203.29

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${IP_VIP}:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

Kết quả sẽ tạo ra file
```
kube-proxy.kubeconfig

```

### Khởi tạo cấu hình cho kube-controller-manager

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```

Kết quả sẽ tạo ra file
```
kube-controller-manager.kubeconfig

```

### Khởi tạo cấu hình cho kube scheduler

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```

Kết quả sẽ tạo ra
```
kube-scheduler.kubeconfig

```


### Khởi tạo cấu hình cho Admin

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```

Kết quả sẽ tạo ra file
```
admin.kubeconfig

```

### Khởi tạo cấu hình cho Calio.

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=cni.kubeconfig

kubectl config set-credentials calico-cni \
  --client-certificate=calio.pem \
  --client-key=calio-key.pem \
  --embed-certs=true \
  --kubeconfig=cni.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=calico-cni \
  --kubeconfig=cni.kubeconfig

kubectl config use-context default --kubeconfig=cni.kubeconfig
```


### Copy cấu hình kubelet, cni-calio và kubeproxy tới các node Worker
```
for instance in worker1 worker2; do
  scp ${instance}.kubeconfig cni.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done
```

### Copy cấu hình kube-controller-manager và kube-scheduler lên các node Master
```
for instance in master1 master2 master3; do
  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done
```



## 4. Khởi tạo  Data Encryption Key

Kube hỗ trợ khả năng mã hóa các cấu hình, trạng thái trong cluster. 
Khởi tạo encrypt key
```
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

```

Khởi tạo Object
```
cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

- Sao chép sang các máy chủ master
```
for instance in master1 master2 master3; do
  scp encryption-config.yaml ${instance}:~/
done
```

## 5. Khởi tạo Etcd Cluster

- Truy cập vào máy chủ master1, master2, master3
```
ssh master1
ssh master2
ssh master3

```

Trên máy chủ này thực hiện tải về Etcd từ Github 
```
wget -q --timestamping \
  "https://github.com/etcd-io/etcd/releases/download/v3.4.0/etcd-v3.4.0-linux-amd64.tar.gz"
```


Giải nén Etcd, và di chuyển vào thư mục binary
```
tar -xvf etcd-v3.4.0-linux-amd64.tar.gz
mv etcd-v3.4.0-linux-amd64/etcd* /usr/local/bin/
```

Khởi tạo thư mục chứa dữ liệu, sao chép TLS Certificate
```
mkdir -p /etc/etcd /var/lib/etcd
cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
```

Khởi tạo Service Etcd. Sử dụng INTERNAL_IP chính là địa chỉ đường Manage của node này, cũng là đường liên hệ giữa cluster
Ngoài ra tại option initial-cluster, thực hiện chỉnh sửa hostname và địa chỉ IP manage của các node master còn lại
```

ETCD_NAME=$(hostname -s)

## chỉnh sửa IP với từng node tương ứng
INTERNAL_IP=10.10.203.30
INTERNAL_IP=10.10.203.31
INTERNAL_IP=10.10.203.52


cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master1=https://10.10.203.31:2380,master2=https://10.10.203.30:2380,master3=https://10.10.203.52:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

Khởi động service Etcd. Lưu ý sẽ start đồng thời serivce trên cả 3 node để thực hiện peering cluster.
```
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
```

Kiểm tra trạng thái Etcd
```
[root@master1 key]#  systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2020-04-09 11:16:09 +07; 13s ago
     Docs: https://github.com/coreos
 Main PID: 16772 (etcd)
   CGroup: /system.slice/etcd.service
           └─16772 /usr/local/bin/etcd --name master1 --cert-file=/etc/etcd/kubernetes.pem --key-file=/etc/etcd/kubernetes-key.pem --peer-cert-file=/etc/etcd/kubernetes.pem --peer-key-file=/etc/etcd/kube...

Apr 09 11:16:09 master1 etcd[16772]: a1e038d56b9d9de9 initialized peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s)
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream Message writer)
Apr 09 11:16:09 master1 etcd[16772]: ready to serve client requests
Apr 09 11:16:09 master1 etcd[16772]: ready to serve client requests
Apr 09 11:16:09 master1 etcd[16772]: published {Name:master1 ClientURLs:[https://10.10.203.31:2379]} to cluster 3239a6a4243cf8e5
Apr 09 11:16:09 master1 systemd[1]: Started etcd.
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream Message reader)
Apr 09 11:16:09 master1 etcd[16772]: serving client requests on 10.10.203.31:2379
Apr 09 11:16:09 master1 etcd[16772]: serving client requests on 127.0.0.1:2379
Apr 09 11:16:09 master1 etcd[16772]: established a TCP streaming connection with peer f90331b1cff48a13 (stream MsgApp v2 reader)

```

Kiểm tra danh sách member có trong cluster Etcd
```

[root@master3 ~]# ETCDCTL_API=3 etcdctl -w table endpoint --cluster status    --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.pem   --cert=/etc/etcd/kubernetes.pem   --key=/etc/etcd/kubernetes-key.pem
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://10.10.203.30:2379 | 128a267aefa911c8 |   3.4.0 |  5.9 MB |      true |      false |       360 |       2454 |               2454 |        |
| https://10.10.203.31:2379 | a1e038d56b9d9de9 |   3.4.0 |  5.9 MB |     false |      false |       360 |       2454 |               2454 |        |
| https://10.10.203.52:2379 | f90331b1cff48a13 |   3.4.0 |  5.9 MB |     false |      false |       360 |       2454 |               2454 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

ETCDCTL_API=3 etcdctl -w json endpoint status   --endpoints=https://127.0.0.1:2379   --cacert=/etc/etcd/ca.pem   --cert=/etc/etcd/kubernetes.pem   --key=/etc/etcd/kubernetes-key.pem

```


## 6. Khởi tạo Kubernetes Control Plane

Danh sách các thành phần trong Control Plane: Kubernetes API Server, Scheduler, and Controller Manager.
Thực hiện trên 3 máy chủ master
```
ssh master1
ssh master2
ssh master3
```


Khởi tạo thư mục chứa file cấu hình
```
mkdir -p /etc/kubernetes/config
```

Tải về binary của các thành phần cần cài đặt
```
wget -q  \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl"
```

Thực hiện phân quyền và copy vào thư mục binary của hệ thống
```
chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
```

###  Cấu hình API Server

```
mkdir -p /var/lib/kubernetes/

mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
  service-account-key.pem service-account.pem \
  encryption-config.yaml /var/lib/kubernetes/
```

Tương ứng với từng máy chủ master, địa chỉ INTERNAI_IP sẽ khác. Ngoài ra cần chú ý thêm địa chỉ của etcd-server chính là địa chỉ các etcd server vừa tạo. service-cluster-ip-range sẽ là địa chỉ của Cluster IP ( sẽ tìm hiểu trong tương lai, hiện tại chỉ cần chọn subnet không cần trùng với mạng vật lý )
```
INTERNAL_IP=10.10.203.30
INTERNAL_IP=10.10.203.31
INTERNAL_IP=10.10.203.52 

```

Cấu hình Service
```
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.10.203.31:2379,https://10.10.203.30:2379,https://10.10.203.52:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```


###  Cấu hình Kube Controller Manager

Sao chép cấu hình kubemanager
```
mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

Khởi tạo kube-controller-manager.service. Chú ý : cluster-cidr sẽ là subnet mà các Pod endpoint được gắn
```
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```


### Cấu hình Configure the Kubernetes Scheduler

Chuyển cấu hình kube-scheduler
```
 mv kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Khởi tạo cấu hình kube-scheduler.yaml

```
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF
```

khởi tạo scheduler service
```
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### Khởi động các dịch vụ

Khởi động các dịch trên controller
```
systemctl daemon-reload
systemctl enable kube-apiserver kube-controller-manager kube-scheduler
systemctl start kube-apiserver kube-controller-manager kube-scheduler
systemctl restart kube-apiserver kube-controller-manager kube-scheduler
systemctl status kube-apiserver kube-controller-manager kube-scheduler

```


### Kiểm tra dịch vụ

Kiểm tra dịch vụ đã xanh lè xanh lét thì tiếp tục làm các bước tiếp theo
Có thể trạng thái đã OK nhưng vẫn xuất hiện lỗi trong quá trình vận hành
Quá trình boostrap và peering giữa các service khá lâu, và xuất hiện lỗi có thể phải chờ 5 - 10 phút để hệ thống hoạt động  ổ định.


### Kiểm tra trạng thái
```
# kubectl get componentstatuses --kubeconfig admin.kubeconfig
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}
[root@master1 ~]#

```


### Câu hình RBAC cho Kubelet
Cấu hình RBAC cho phép  Kubernetes API Server access vào Kubelet API trên các worker để lấy metric, log và chạy command
```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF
```

Bind system:kube-apiserver-to-kubeletClusterRole  cho User kubernetes 
```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```

### Cấu hình RBAC cho  CNI plugin - Calio để truy cập vào Kubenetes
```
kubectl apply --kubeconfig admin.kubeconfig -f - <<EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-cni
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  # The CNI plugin patches pods/status.
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
 # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - ipamconfigs
      - clusterinformations
      - ippools
    verbs:
      - get
      - list
EOF
```

Binding Role
```
kubectl create clusterrolebinding calico-cni --clusterrole=calico-cni --user=calico-cni
```

## 6. Khởi tạo Kube Worker node

### Cài đặt các gói yêu cầu, Runtime env

Cài gói yêu cầu
```
yum install -y  socat conntrack ipset
```

Kube sẽ bị lỗi khi sử dụng swap, disable swap trên Worker
```
swapon --show 
swapoff -a
 
```

Cài đặt Docker để làm Runtime Env. Có thể lựa chọn các giải pháp khác
```bash
# Install Docker CE
## Set up the repository
### Install required packages.
yum install -y yum-utils device-mapper-persistent-data lvm2

### Add Docker repository.
yum-config-manager --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo

## Install Docker CE.
yum update -y && yum install -y \
  containerd.io-1.2.13 \
  docker-ce-19.03.8 \
  docker-ce-cli-19.03.8

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker

```

- Kiểm tra cgroup

```
docker info | grep -i cgroup
 Cgroup Driver: systemd

```


Tải về và cài đặt các binary cần thiết
```
wget -q \
https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubelet \
  https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz 

```



Khởi tạo thư mục
```
sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```


Cài đặt các binary
```
tar -xvf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin/
chmod +x  kubectl kube-proxy kubelet  
mv kubectl kube-proxy kubelet /usr/local/bin/
```



### Cấu hình Kubelet
```
mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
mv ca.pem /var/lib/kubernetes/
```


Khởi tạo kubelet-config.yaml
Trong đó clusterDNS chỉnh địa chỉ của Cluster IP ( service-cluster-ip-range ) của CoreDNS sẽ khởi tạo sau.
POD_CIDR chính là subnet cluster-cidr ( CIDR Range for Pods in cluster) : 10.200.0.0/16 

POD_CIDR=10.200.0.0/16 

```
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
```

Cấu hình file service cho kubelet
```
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=5m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --cgroup-driver=systemd \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### Cấu hình Kube-proxy

Sao chép cấu hình
```
 mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
```

Khởi tạo Object
```

POD_CIDR=10.200.0.0/16 

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "${POD_CIDR}"
EOF
```

Khởi tạo cấu hình systemd
```
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

```
### Khởi động Kubelet và Kube-proxy
```
systemctl daemon-reload
systemctl enable docker kubelet kube-proxy
systemctl stop docker kubelet kube-proxy
systemctl start docker kubelet kube-proxy

systemctl status docker kubelet kube-proxy

```

### Xem danh sách các node
```
[root@master1 ~]# kubectl get nodes --kubeconfig admin.kubeconfig
NAME      STATUS   ROLES    AGE   VERSION
worker1   Ready    <none>   11m   v1.18.0
worker2   Ready    <none>   30s   v1.18.0

```
### Cài đặt Calio


### Thực hiện cấu hình CNI trên các máy Worker
Cấu hình Calio
```
curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/v3.8.0/calico-amd64
chmod 755 /opt/cni/bin/calico
curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/v3.8.0/calico-ipam-amd64
chmod 755 /opt/cni/bin/calico-ipam
```

Khởi tạo thưc mục cấu hình
```
mkdir -p /etc/cni/net.d/
```

Copy cấu hình cni-Calio
```
cp cni.kubeconfig  /var/lib/kubernetes/
chmod 600 /var/lib/kubernetes/cni.kubeconfig
```

Khởi tạo cấu hình CNI
```
cat > /etc/cni/net.d/10-calico.conflist <<EOF
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "datastore_type": "kubernetes",
      "mtu": 1500,
      "ipam": {
          "type": "calico-ipam"
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "kubeconfig": "/var/lib/kubernetes/cni.kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    }
  ]
}
EOF
```


#### Cài đặt Calio Typha trêm các máy Master

Khởi tạo Certificate cho Typha 
```
cat > typha-csr.json <<EOF
{
  "CN": "system:typha",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "VI",
      "L": "HANOI",
      "O": "system:typha",
      "OU": "Kubernetes The Hard Way",
      "ST": "Scheduler"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  typha-csr.json | cfssljson -bare typha
```

Khởi tạo Service Account
```
kubectl create serviceaccount -n kube-system calico-typha
```


Khởi tạo typa config

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=typha.kubeconfig

kubectl config set-credentials calico-typha \
  --client-certificate=typha.pem \
  --client-key=typha-key.pem \
  --embed-certs=true \
  --kubeconfig=typha.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=calico-typha \
  --kubeconfig=typha.kubeconfig

kubectl config use-context default --kubeconfig=typha.kubeconfig
```


Khởi tạo Secret và gắn certificate vào server
```
kubectl create secret generic -n kube-system calico-typha-certs --from-file=typha-key.pem --from-file=typha.pem

```

Khởi tạo Service Account
```
export KUBECONFIG=admin.kubeconfig
kubectl create  serviceaccount -n kube-system calico-typha
```

Khởi tạo RBAC service account 
```
kubectl apply -f - <<EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: calico-typha
rules:
  - apiGroups: [""]
    resources:
      - pods
      - namespaces
      - serviceaccounts
      - endpoints
      - services
      - nodes
    verbs:
      # Used to discover service IPs for advertisement.
      - watch
      - list
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - ipamblocks
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
      - blockaffinities
      - networksets
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      #- ippools
      #- felixconfigurations
      - clusterinformations
    verbs:
      - get
      - create
      - update
EOF
```

Bind RBAC vào account
```
kubectl create clusterrolebinding calico-typha --clusterrole=calico-typha --serviceaccount=kube-system:calico-typha

```


Khởi tạo Deployment typha
```
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  replicas: 2
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      k8s-app: calico-typha
  template:
    metadata:
      labels:
        k8s-app: calico-typha
      annotations:
        # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
        # add-on, ensuring it gets priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      hostNetwork: true
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      serviceAccountName: calico-typha
      priorityClassName: system-cluster-critical
      containers:
      - image: calico/typha:v3.8.0
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        env:
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: "none"
          - name: TYPHA_LOGSEVERITYSYS
            value: "none"
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: "kubernetes"
          - name: TYPHA_DATASTORETYPE
            value: "kubernetes"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          # Location of the CA bundle Typha uses to authenticate calico/node; volume mount
          - name: TYPHA_CAFILE
            value: /calico-typha-ca/typhaca.crt
          # Common name on the calico/node certificate
          - name: TYPHA_CLIENTCN
            value: calico-node
          # Location of the server certificate for Typha; volume mount
          - name: TYPHA_SERVERCERTFILE
            value: /calico-typha-certs/typha.crt
          # Location of the server certificate key for Typha; volume mount
          - name: TYPHA_SERVERKEYFILE
            value: /calico-typha-certs/typha.key
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9098
            host: localhost
          periodSeconds: 30
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9098
            host: localhost
          periodSeconds: 10
        volumeMounts:
        - name: calico-typha-ca
          mountPath: "/calico-typha-ca"
          readOnly: true
        - name: calico-typha-certs
          mountPath: "/calico-typha-certs"
          readOnly: true
      volumes:
      - name: calico-typha-ca
        configMap:
          name: calico-typha-ca
      - name: calico-typha-certs
        secret:
          secretName: calico-typha-certs
EOF
```

Xem danh sách các Pods đã khởi tạo
```
kubectl get pods -l k8s-app=calico-typha -n kube-system
```